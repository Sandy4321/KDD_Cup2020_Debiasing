{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have not glove\n",
      "have not glove\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "# 内存回收\n",
    "import gc\n",
    "# 模型\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool, cpu_count\n",
    "# 代码运行耗时查看器\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from recall import topk_recall_association_rules_qyxs_icf, topk_recall_association_rules_ucf\n",
    "from process import load_click_data, data_generate, item_cluster_feat, user_cluster_feat, \\\n",
    "                     matrix_word2vec_embedding, embedding_fea_pca, get_train_test_data, \\\n",
    "                     Pool_feature_concat, phase_submit_save, add_time_statistics, \\\n",
    "                     add_user_statistics, add_item_statistics, load_pool_feature, \\\n",
    "                     remove_pool_feature, user_feat_data, age_sex_city_data, null_xgb_predict, add_user_profile_info\n",
    "from model import train_model_lgb\n",
    "from metric import metrics_recall \n",
    "\n",
    "train_path = '../data/underexpose_train'  \n",
    "test_path = '../data/underexpose_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容\n",
    "\n",
    "- 相对于V1增加了用户最喜爱session，用户不同session数\n",
    "- item_n_clusters、user_n_clusters改为50\n",
    "- 用户画像信息（但是重要性很低）\n",
    "- lgb模型metric改为\"MAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "phase_append = False\n",
    "flag_test = False\n",
    "icf_recall_num = 700\n",
    "ucf_recall_num = 500\n",
    "topk = 50\n",
    "# 构造特征时取用户最近点击项目数\n",
    "click_topn=5\n",
    "# pca成分数量\n",
    "pca_n_components=5\n",
    "# 项目聚类数\n",
    "item_n_clusters=20\n",
    "# 用户聚类数\n",
    "user_n_clusters=20\n",
    "nrows = None \n",
    "# embedding特征与召回数据已有，直接取为True\n",
    "topk_recall_read = True\n",
    "embedding_feat_read = True\n",
    "# 进程处理中数据划分份数或者任务数量\n",
    "batchs_n = 4\n",
    "# 进程中open的cpu核数\n",
    "open_cpu_n = 5\n",
    "\n",
    "\n",
    "\n",
    "# test\n",
    "# phase_append = False\n",
    "# flag_test = True\n",
    "# recall_num = 50\n",
    "# topk = 50\n",
    "# nrows = 1000\n",
    "\n",
    "# txt与img余弦相似度字典\n",
    "txt_cosine_similarity_dict = np.load('../data/process/txt_cosine_similarity_dict.npy', allow_pickle=True).item()\n",
    "img_cosine_similarity_dict = np.load('../data/process/img_cosine_similarity_dict.npy', allow_pickle=True).item()\n",
    "type(txt_cosine_similarity_dict['42844']['67898'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 7\n",
      "================================== 加载click数据 ==================================\n",
      "================================== 生成中间数据 ==================================\n",
      "==================================== 召回 ====================================\n",
      "-------- 评测召回效果 -------------\n",
      "-------- 召回效果 -------------\n",
      "--------:phase:  7  -------------\n",
      "phase:  7  top_ 0  :  hit_num :  303 hit_rate :  0.016829593423683627  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 70  :  hit_num :  2324 hit_rate :  0.1290824261275272  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 140  :  hit_num :  2897 hit_rate :  0.16090868695845367  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 210  :  hit_num :  3211 hit_rate :  0.1783492557209509  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 280  :  hit_num :  3406 hit_rate :  0.1891801821817374  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 350  :  hit_num :  3575 hit_rate :  0.19856698511441903  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 420  :  hit_num :  3711 hit_rate :  0.20612086203065985  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 490  :  hit_num :  3844 hit_rate :  0.21350810930904243  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 560  :  hit_num :  3959 hit_rate :  0.2198955787602755  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 630  :  hit_num :  4063 hit_rate :  0.22567207287269495  data_num :  18004\n",
      "\n",
      "phase:  7  top_ 700  :  hit_num :  4143 hit_rate :  0.2301155298822484  data_num :  18004\n",
      "\n",
      "召回TOP:700时, 命中百分比:0.2301155298822484\n",
      "==================================== 排序 ====================================\n",
      "-------- 构建特征 ---------\n",
      "-------- sku1 sku2 sku3 sku4 sku5 user ----------\n",
      "------- user1 user2 user3 user4 user5 sku -------\n",
      "------- item1 item2 item3 item4 item5 -------\n",
      "------- user1 user2 user3 user4 user5 -------\n",
      "------- Interactive Embedding Feature PCA -----------\n",
      "------- 降成5维后的方差百分比:  [0.08623923 0.07723182 0.06292848 0.0542458  0.05171798]\n",
      "------- 降成5维后的方差百分比:  [0.06584084 0.05839636 0.05068813 0.04458261 0.0414649 ]\n",
      "------- 降成5维后的方差百分比:  [0.06332094 0.05623574 0.04956183 0.04555777 0.04238139]\n",
      "------- 降成5维后的方差百分比:  [0.09712362 0.07883415 0.07342834 0.06585905 0.05099656]\n",
      "dict_embedding_all_ui_user是否降维成功........ 5\n",
      "------- user or item Embedding Feature Cluster -----------\n",
      "------- 特征加工 -----------\n",
      "..............topk_recall shape: (13860700, 5)\n",
      "------- 构建样本 -----------\n",
      "............... train len= 756000\n",
      "............... test  len= 316400\n",
      "----------- 加入特征 train -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14358it [00:27, 519.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- 构建样本 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16098it [00:31, 519.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............... train len= 723275\n",
      "............... test  len= 310100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16203it [00:31, 519.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 加入特征 train -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30832it [00:59, 513.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- 构建样本 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16372it [00:31, 515.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............... train len= 667800\n",
      "............... test  len= 321300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32652it [01:03, 516.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 加入特征 train -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14627it [00:29, 437.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- 构建样本 -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49390it [01:36, 513.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............... train len= 752500\n",
      "............... test  len= 310100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49494it [01:37, 514.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 加入特征 train -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "420786it [14:38, 511.66it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "now_phase = 7\n",
    "\n",
    "\n",
    "for phase in range(now_phase, now_phase + 1):  \n",
    "    print('phase:', phase)\n",
    "    \"\"\"\n",
    "    train_test: 训练测试点击数据\n",
    "    item_hot_list: 项目热度list\n",
    "    dict_label_user_item: 训练点击中用户——最后一次点击项目字典\n",
    "    item_hot_dict: 项目热度字典\n",
    "    time_feat：用户点击时间特征\n",
    "    \n",
    "    \"\"\"\n",
    "    click_all, test_user_set = load_click_data(phase, nrows=nrows)\n",
    "    train_test, item_hot_list, dict_label_user_item, item_hot_dict, time_feat = data_generate(click_all, test_user_set)\n",
    "\n",
    "\n",
    "    \n",
    "    print('==================================== 召回 ====================================')\n",
    "    \n",
    "    # TODO: 增加其它召回\n",
    "    if topk_recall_read:\n",
    "        topk_recall_icf = pd.read_pickle('../data/recall/topk_recall_icf_{phase}.pkl'.format(phase=phase))\n",
    "        matrix_association_rules = np.load('../data/recall/matrix_association_rules_{phase}.npy'.format(phase=phase), allow_pickle=True).item()\n",
    "        topk_recall_ucf = pd.read_pickle('../data/recall/topk_recall_ucf_{phase}.pkl'.format(phase=phase))\n",
    "        sim_user_corr = np.load('../data/recall/sim_user_corr_{phase}.npy'.format(phase=phase), allow_pickle=True).item()             \n",
    "        \n",
    "    else:\n",
    "        topk_recall_icf, matrix_association_rules = topk_recall_association_rules_qyxs_icf(\n",
    "                                                click_all=train_test,\n",
    "                                                dict_label=dict_label_user_item, \n",
    "                                                k=icf_recall_num\n",
    "                                                )\n",
    "        topk_recall_icf.to_pickle('../data/recall/topk_recall_icf_{phase}.pkl'.format(phase=phase))\n",
    "        np.save('../data/recall/matrix_association_rules_{phase}.npy'.format(phase=phase), matrix_association_rules)\n",
    "\n",
    "        topk_recall_ucf, sim_user_corr = topk_recall_association_rules_ucf(click_all=train_test, \n",
    "                                                                       dict_label=dict_label_user_item, \n",
    "                                                                       k=ucf_recall_num)\n",
    "        topk_recall_ucf.to_pickle('../data/recall/topk_recall_ucf_{phase}.pkl'.format(phase=phase))\n",
    "        np.save('../data/recall/sim_user_corr_{phase}.npy'.format(phase=phase), sim_user_corr)\n",
    "\n",
    "    # recall融合\n",
    "    topk_recall = topk_recall_icf.merge(topk_recall_ucf, on=['user_id', 'item_similar', 'next_item_id', 'pred'], how='outer')\n",
    "    \n",
    "    # recall融合\n",
    "    topk_recall = topk_recall_icf.merge(topk_recall_ucf, on=['user_id', 'item_similar', 'next_item_id', 'pred'], how='outer')\n",
    "    \n",
    "    # 缺失填补\n",
    "    topk_recall = topk_recall.fillna(0)\n",
    "    # 热品得分0化存在问题（这样使得数据会存在问题，不具有差异性）\n",
    "    # topk_recall.loc[topk_recall.score_similar_x<0, 'score_similar_x'] = 0\n",
    "    # topk_recall.loc[topk_recall.score_similar_y<0, 'score_similar_y'] = 0\n",
    "    \n",
    "    # 标准化\n",
    "    # 因为数据取值差异较大，所有在这里只对部分取值[0,5)做了归一化\n",
    "    score_similar_x=np.array(topk_recall[(topk_recall.score_similar_x>=0) & (topk_recall.score_similar_x<5)]['score_similar_x']).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    topk_recall.loc[(topk_recall.score_similar_x>=0) & (topk_recall.score_similar_x<5), 'score_similar_x'] = scaler.fit_transform(score_similar_x).flatten()\n",
    "    \n",
    "    score_similar_y=np.array(topk_recall[(topk_recall.score_similar_y>=0) & (topk_recall.score_similar_y<5)]['score_similar_y']).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    topk_recall.loc[(topk_recall.score_similar_y>=0) & (topk_recall.score_similar_y<5), 'score_similar_y'] = scaler.fit_transform(score_similar_y).flatten()\n",
    "    \n",
    "    # 多路召回得分融合\n",
    "    topk_recall['score_similar'] = (2*topk_recall['score_similar_x'] + 1*topk_recall['score_similar_y'])/3\n",
    "    topk_recall.drop(['score_similar_x', 'score_similar_y'], axis=1, inplace=True)\n",
    "    \n",
    "    # 每个用户得分降排，并取多路的top topn\n",
    "    topn = topk_recall.groupby(['user_id'])['item_similar'].nunique().min()\n",
    "    topk_recall.sort_values(['user_id', 'score_similar'], ascending=[1,0], inplace=True)\n",
    "    topk_recall = topk_recall.groupby(['user_id']).head(topn)\n",
    "    \n",
    "        \n",
    "    print('-------- 评测召回效果 -------------')\n",
    "    hit_rate = metrics_recall(topk_recall=topk_recall, phase=phase, k=topn, sep=int(topn/10))\n",
    "    print('召回TOP:{k}时, 命中百分比:{hit_rate}'.format(k=topn, hit_rate=hit_rate))\n",
    "\n",
    "    \n",
    "    \n",
    "    print('==================================== 排序 ====================================')\n",
    "    print('-------- 构建特征 ---------')\n",
    "    \n",
    "    dim, epochs, learning_rate = 30, 15, 0.1 # 由于内存问题，size改回30\n",
    "    if embedding_feat_read:\n",
    "        print('-------- sku1 sku2 sku3 sku4 sku5 user ----------')\n",
    "        dict_embedding_all_ui_item = np.load('../data/embedding/dict_embedding_all_ui_item_{phase}.npy'.format(phase=phase), \n",
    "                                             allow_pickle=True).item()\n",
    "        \n",
    "        print('------- user1 user2 user3 user4 user5 sku -------')\n",
    "        dict_embedding_all_ui_user = np.load('../data/embedding/dict_embedding_all_ui_user_{phase}.npy'.format(phase=phase), \n",
    "                                             allow_pickle=True).item()\n",
    "        \n",
    "        print('------- item1 item2 item3 item4 item5 -------')\n",
    "        dict_embedding_item_only = np.load('../data/embedding/dict_embedding_item_only_{phase}.npy'.format(phase=phase), \n",
    "                                           allow_pickle=True).item()\n",
    "        \n",
    "        print('------- user1 user2 user3 user4 user5 -------')\n",
    "        dict_embedding_user_only = np.load('../data/embedding/dict_embedding_user_only_{phase}.npy'.format(phase=phase), \n",
    "                                           allow_pickle=True).item()\n",
    "\n",
    "    else:\n",
    "        print('-------- sku1 sku2 sku3 sku4 sku5 user ----------')\n",
    "        dict_embedding_all_ui_item = matrix_word2vec_embedding(\n",
    "                                                                click_all=train_test,\n",
    "                                                                flag='item',\n",
    "                                                                mode='all',\n",
    "                                                                dim=dim,\n",
    "                                                                epochs=epochs,\n",
    "                                                                learning_rate=learning_rate\n",
    "                                                                )\n",
    "        np.save('../data/embedding/dict_embedding_all_ui_item_{phase}.npy'.format(phase=phase),\n",
    "               dict_embedding_all_ui_item)\n",
    "\n",
    "\n",
    "        print('------- user1 user2 user3 user4 user5 sku -------')\n",
    "        dict_embedding_all_ui_user = matrix_word2vec_embedding(\n",
    "                                                                click_all=train_test,\n",
    "                                                                flag='user',\n",
    "                                                                mode='all',\n",
    "                                                                dim=dim,\n",
    "                                                                epochs=epochs,\n",
    "                                                                learning_rate=learning_rate\n",
    "                                                                )\n",
    "        np.save('../data/embedding/dict_embedding_all_ui_user_{phase}.npy'.format(phase=phase),\n",
    "               dict_embedding_all_ui_user)\n",
    "\n",
    "\n",
    "        print('------- item1 item2 item3 item4 item5 -------')\n",
    "        dict_embedding_item_only = matrix_word2vec_embedding(\n",
    "                                                                click_all=train_test,\n",
    "                                                                flag='item',\n",
    "                                                                mode='only',\n",
    "                                                                dim=dim,\n",
    "                                                                epochs=epochs,\n",
    "                                                                learning_rate=learning_rate\n",
    "                                                                )\n",
    "        np.save('../data/embedding/dict_embedding_item_only_{phase}.npy'.format(phase=phase),\n",
    "               dict_embedding_item_only)\n",
    "\n",
    "\n",
    "        print('------- user1 user2 user3 user4 user5 -------')\n",
    "        dict_embedding_user_only = matrix_word2vec_embedding(\n",
    "                                                                click_all=train_test,\n",
    "                                                                flag='user',\n",
    "                                                                mode='only',\n",
    "                                                                dim=dim,\n",
    "                                                                epochs=epochs,\n",
    "                                                                learning_rate=learning_rate\n",
    "                                                                )\n",
    "        np.save('../data/embedding/dict_embedding_user_only_{phase}.npy'.format(phase=phase),\n",
    "               dict_embedding_user_only)\n",
    "\n",
    "        \n",
    "    \n",
    "    print('------- Interactive Embedding Feature PCA -----------')\n",
    "\n",
    "    # dict_embedding_all_ui_item PCA\n",
    "    dict_embedding = dict_embedding_all_ui_item.copy()\n",
    "    dict_embedding_all_ui_item = {}\n",
    "    for i in ['user', 'item']:\n",
    "        tmp = pd.DataFrame(dict_embedding[i])\n",
    "        pca_ = embedding_fea_pca(tmp.T, n_components=pca_n_components) # 解释\n",
    "        tmp_pca = pd.DataFrame(pca_.T)\n",
    "        tmp_pca.columns = tmp.columns\n",
    "        dict_embedding_all_ui_item[i] = dict(tmp_pca)\n",
    "    \n",
    "    \n",
    "    # dict_embedding_all_ui_user PCA\n",
    "    dict_embedding = dict_embedding_all_ui_user.copy()\n",
    "    dict_embedding_all_ui_user = {}\n",
    "    for i in ['user', 'item']:\n",
    "        tmp = pd.DataFrame(dict_embedding[i])\n",
    "        pca_ = embedding_fea_pca(tmp.T, n_components=pca_n_components) # 解释\n",
    "        tmp_pca = pd.DataFrame(pca_.T)\n",
    "        tmp_pca.columns = tmp.columns\n",
    "        dict_embedding_all_ui_user[i] = dict(tmp_pca)\n",
    "    print('dict_embedding_all_ui_user是否降维成功........', len(dict_embedding_all_ui_user['user']['1']))    \n",
    "    del tmp, tmp_pca\n",
    "\n",
    "    \n",
    "    \n",
    "    print('------- user or item Embedding Feature Cluster -----------')\n",
    "\n",
    "    # dict_embedding_item_only聚类\n",
    "    item = pd.DataFrame(dict_embedding_item_only['item'])\n",
    "    km_cluster = KMeans(n_clusters=item_n_clusters, max_iter=300, n_init=40, \\\n",
    "                        init='k-means++') #, n_jobs=-1)\n",
    "    # 返回类索引\n",
    "    result = km_cluster.fit_predict(item.T)\n",
    "    item_cluster = pd.DataFrame({'item_id': item.columns, 'item_cluster': np.int16(result)})\n",
    "    del item, km_cluster, result, dict_embedding_item_only\n",
    "    \n",
    "    # dict_embedding_user_only聚类\n",
    "    user = pd.DataFrame(dict_embedding_user_only['user'])\n",
    "    km_cluster = KMeans(n_clusters=user_n_clusters, max_iter=300, n_init=40, \\\n",
    "                        init='k-means++') #, n_jobs=-1)\n",
    "    # 返回类索引\n",
    "    result = km_cluster.fit_predict(user.T)\n",
    "    user_cluster = pd.DataFrame({'user_id': user.columns, 'user_cluster': np.int16(result)})\n",
    "    del user, km_cluster, result\n",
    "    \n",
    "    \n",
    "    # 加载聚类的统计特征\n",
    "    user_cluster_feat_ = user_cluster_feat(click_all, user_cluster)\n",
    "    item_cluster_feat_ = item_cluster_feat(click_all, item_cluster)\n",
    "\n",
    "    del click_all, item_cluster\n",
    "    \n",
    "    \n",
    "    print('------- 特征加工 -----------')\n",
    "\n",
    "    print('..............topk_recall shape:', topk_recall.shape)\n",
    "\n",
    "    \n",
    "#     feature_all = get_train_test_data(topk_recall,\n",
    "#                   matrix_association_rules, \n",
    "#                   time_feat,\n",
    "#                   user_cluster_feat_,\n",
    "#                   item_cluster_feat_,\n",
    "#                   item_hot_dict,\n",
    "#                   txt_cosine_similarity_dict,\n",
    "#                   img_cosine_similarity_dict,\n",
    "#                   dict_embedding_all_ui_item,\n",
    "#                   dict_embedding_all_ui_user,\n",
    "#                   flag_test,\n",
    "#                   click_topn)\n",
    "\n",
    "\n",
    "#     lprofiler = LineProfiler(get_train_test_data)\n",
    "#     lprofiler.run('get_train_test_data( topk_recall[:80000], matrix_association_rules, time_feat, user_cluster_feat_, item_cluster_feat_, item_hot_dict, txt_cosine_similarity_dict, img_cosine_similarity_dict, dict_embedding_all_ui_item, dict_embedding_all_ui_user, flag_test=False, click_topn=click_topn)')\n",
    "#     lprofiler.print_stats()\n",
    "\n",
    "    \n",
    "    # 内存回收\n",
    "    gc.collect()\n",
    "    # 多进程构建特征\n",
    "    # 函数get_train_test_data内部已保存结果\n",
    "    if __name__=='__main__':\n",
    "        cpu_num = cpu_count()\n",
    "        \n",
    "        pool = Pool(open_cpu_n+1) # 创建一个多个进程的进程池\n",
    "        # 将topk_recall分batchs_n批次，并不是越大越好，启动进程也是需时间的\n",
    "        batch_size = topk_recall.shape[0]//(batchs_n)\n",
    "        \n",
    "        for batch in range(batchs_n):\n",
    "            start_index = batch * batch_size\n",
    "            if batch==cpu_num:\n",
    "                end_index = topk_recall.shape[0]\n",
    "            else:\n",
    "                end_index = (1+batch) * batch_size\n",
    "            p = pool.apply_async(func=get_train_test_data, \n",
    "                                                 args=(topk_recall[start_index:end_index],\n",
    "                                                       matrix_association_rules, \n",
    "                                                       time_feat,\n",
    "                                                       user_cluster_feat_,\n",
    "                                                       item_cluster_feat_,\n",
    "                                                       item_hot_dict,\n",
    "                                                       txt_cosine_similarity_dict,\n",
    "                                                       img_cosine_similarity_dict,\n",
    "                                                       dict_embedding_all_ui_item,\n",
    "                                                       dict_embedding_all_ui_user,\n",
    "                                                       phase,\n",
    "                                                       batch,\n",
    "                                                       flag_test,\n",
    "                                                       click_topn))            \n",
    "        pool.close()\n",
    "        pool.join() \n",
    "        \n",
    "        \"\"\"\n",
    "        遍历result列表，取出子进程对象，访问get()方法，获取返回值。（此时所有子进程已执行完毕）\n",
    "        result[0].get()\n",
    "        \"\"\"\n",
    "        print('进程结束') \n",
    "     \n",
    "    \n",
    "    \n",
    "#     # 释放内存\n",
    "# #     del topk_recall, matrix_association_rules, txt_cosine_similarity_dict, img_cosine_similarity_dict, dict_embedding_all_ui_item, dict_embedding_all_ui_user, dict_embedding_item_only, dict_embedding_user_only\n",
    "      \n",
    "    print('分片特征数据拼接')\n",
    "    # feature_all = Pool_feature_concat(feature_part)\n",
    "    # 加载暂存的特征数据\n",
    "    feature_all = load_pool_feature(phase=phase, batchs_n=batchs_n)\n",
    "    # 删除暂存的特征数据\n",
    "    remove_pool_feature(phase, batchs_n)\n",
    "    \n",
    "    \n",
    "    # 用户画像特征缺失预测\n",
    "    user_feat = user_feat_data(dict_embedding_user_only, user_cluster, time_feat)\n",
    "    for i, col in enumerate(['age', 'sex', 'city']):\n",
    "        X_, y_, X_test, data_ = age_sex_city_data(user_feat, col=col)\n",
    "        tmp = null_xgb_predict(X_, y_, X_test, data_, col=col)\n",
    "        if i==0:\n",
    "            user_profile_feat = tmp\n",
    "        else:\n",
    "            user_profile_feat = user_profile_feat.merge(tmp, on=['user_id'], how='left')    \n",
    "\n",
    "    # 增加用户画像特征、时间统计特征、用户聚类统计特征、项目聚类统计特征 \n",
    "    feature_all = add_user_profile_info(feature_all, user_profile_feat)\n",
    "    feature_all = add_time_statistics(feature_all, time_feat)\n",
    "    feature_all = add_user_statistics(feature_all, user_cluster_feat_)\n",
    "    feature_all = add_item_statistics(feature_all, item_cluster_feat_)\n",
    "    print('feature_all的列: ', feature_all.columns)\n",
    "    \n",
    "    \n",
    "#     # 数据量太大了，保存占用空间\n",
    "#     print('特征数据保存')\n",
    "#     feature_all.to_pickle(\"../data/process/feature_all_phase{phase}_0002.pkl\".format(phase=phase))\n",
    "    \n",
    "    # 内存回收\n",
    "    gc.collect()\n",
    "    ############################## 整体数据特征 ############################## \n",
    "    print('--------------------------- 特征数据 ---------------------')\n",
    "    len_f = len(feature_all)\n",
    "    len_train = len(feature_all[feature_all['train_flag']=='train'])\n",
    "    len_test = len(feature_all[feature_all['train_flag']=='test'])\n",
    "    len_train_1 = len(feature_all[(feature_all['train_flag']=='train') & (feature_all['label']== 1)]) \n",
    "    print('所有数据条数', len_f)\n",
    "    print('训练数据 : ', len_train)\n",
    "    print('训练数据 label 1 : ', len_train_1)\n",
    "    print('训练数据 1 / 0 rate : ', len_train_1 * 1.0 / len_f)\n",
    "    print('测试数据 : ' , len_test)\n",
    "    print('flag : ', set(feature_all['train_flag']))\n",
    "\n",
    "    \n",
    "    ############################## 训练模型 ############################## \n",
    "    print('--------------------------- 训练模型 ---------------------')\n",
    "    submit = train_model_lgb(feature_all, recall_rate=hit_rate, hot_list=item_hot_list, valid=0.2, topk=50, \n",
    "                             num_boost_round=1500, early_stopping_rounds=500)\n",
    "\n",
    "    \n",
    "    # submit = train_model_rf(feature_all, recall_rate=hit_rate, hot_list=item_hot_list, valid=0.2, topk=50)\n",
    "    \n",
    "\n",
    "    print('--------------------------- 保存预测文件 ---------------------')\n",
    "    phase_submit_save(submit, phase, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
